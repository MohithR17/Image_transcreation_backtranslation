"""
Helper script to evaluate images from metadata CSV files generated by I2I_trancreation.py
"""

import pandas as pd
import argparse
import yaml
import json
import logging
from qwen_vl_judge import QwenVLJudge
import os


def main():
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    
    parser = argparse.ArgumentParser(
        description="Evaluate images from metadata CSV using Qwen-VL judge"
    )
    parser.add_argument("--metadata", type=str, required=True,
                       help="Path to metadata.csv file")
    parser.add_argument("--config", type=str, required=True,
                       help="Path to evaluation config YAML")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2-VL-7B-Instruct",
                       help="Qwen-VL model to use")
    parser.add_argument("--output", type=str, required=True,
                       help="Output path for evaluation results")
    parser.add_argument("--filter-status", type=str, default="success",
                       help="Only evaluate images with this status (default: success)")
    parser.add_argument("--column", type=str, default="tgt_image_path",
                       help="Column containing image paths to evaluate (default: tgt_image_path)")
    
    args = parser.parse_args()
    
    # Load metadata
    logging.info(f"Loading metadata from {args.metadata}")
    df = pd.read_csv(args.metadata)
    logging.info(f"Total rows in metadata: {len(df)}")
    
    # Filter by status
    if args.filter_status:
        df = df[df['status'] == args.filter_status]
        logging.info(f"Filtered to {len(df)} rows with status '{args.filter_status}'")
    
    # Get image paths
    image_paths = df[args.column].dropna().tolist()
    
    if len(image_paths) == 0:
        logging.error("No images found to evaluate!")
        return
    
    logging.info(f"Found {len(image_paths)} images to evaluate")
    
    # Load evaluation config
    with open(args.config) as f:
        config = yaml.safe_load(f)
    
    # Override image_paths in config
    config['image_paths'] = image_paths
    
    # Initialize judge
    judge = QwenVLJudge(model_name=args.model)
    
    # Create evaluation prompt
    from qwen_vl_judge import EVALUATION_TEMPLATES
    
    if "template" in config and config["template"] in EVALUATION_TEMPLATES:
        template = EVALUATION_TEMPLATES[config["template"]]
        context = template["context"].format(**config.get("template_vars", {}))
        prompt = judge.create_evaluation_prompt(
            task_description=template["task_description"],
            evaluation_criteria=template["criteria"],
            scoring_scale=template["scale"],
            additional_context=context
        )
    else:
        prompt = config.get("prompt", config.get("evaluation_prompt"))
    
    logging.info(f"Evaluation prompt:\n{prompt}\n")
    
    # Run evaluation
    results = judge.batch_evaluate(
        image_paths=image_paths,
        prompts=prompt,
        save_path=args.output,
        max_tokens=config.get("max_tokens", 512),
        temperature=config.get("temperature", 0.1)
    )
    
    # Merge with metadata
    results_df = pd.DataFrame(results)
    results_df = results_df.rename(columns={"image_path": args.column})
    
    # Merge on image path
    merged_df = df.merge(
        results_df[[args.column, 'score', 'justification', 'raw_response']],
        on=args.column,
        how='left'
    )
    
    # Save merged results
    merged_output = args.output.replace('.json', '_with_metadata.csv')
    merged_df.to_csv(merged_output, index=False)
    logging.info(f"Merged results saved to {merged_output}")
    
    # Print summary
    scores = [r["score"] for r in results if r["score"] is not None]
    if scores:
        logging.info(f"\n{'='*50}")
        logging.info(f"Evaluation Summary:")
        logging.info(f"  Total images: {len(results)}")
        logging.info(f"  Successfully evaluated: {len(scores)}")
        logging.info(f"  Average score: {sum(scores)/len(scores):.2f}")
        logging.info(f"  Min score: {min(scores):.2f}")
        logging.info(f"  Max score: {max(scores):.2f}")
        
        # Breakdown by source country if available
        if 'src_country' in merged_df.columns:
            logging.info(f"\n  Scores by source country:")
            country_scores = merged_df.groupby('src_country')['score'].agg(['mean', 'count'])
            for country, row in country_scores.iterrows():
                logging.info(f"    {country}: {row['mean']:.2f} (n={int(row['count'])})")
        
        # Breakdown by category if available
        if 'src_category' in merged_df.columns:
            logging.info(f"\n  Scores by category:")
            category_scores = merged_df.groupby('src_category')['score'].agg(['mean', 'count'])
            for category, row in category_scores.iterrows():
                logging.info(f"    {category}: {row['mean']:.2f} (n={int(row['count'])})")
        
        logging.info(f"{'='*50}\n")


if __name__ == "__main__":
    main()
